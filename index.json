[{"authors":null,"categories":null,"content":"","date":1755216e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1755216e3,"objectID":"4a6b6ed47cf4f61536d7dc1ac9e0a4de","permalink":"","publishdate":"2025-08-15T00:00:00Z","relpermalink":"","section":"publication","summary":"**Amr Mousa**, Neil Karavis, Michele Caprio, Wei Pan, Richard Allmendinger","tags":[],"title":"TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion","type":"publication"},{"authors":null,"categories":null,"content":"","date":1754006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754006400,"objectID":"7f2085fdde4edee58495e95bb3442e1c","permalink":"","publishdate":"2025-08-01T00:00:00Z","relpermalink":"","section":"news","summary":"I will be presenting our paper at IROS 2025.\nSee the [üìÑ paper](https://arxiv.org/pdf/2503.20839), [üíª code](https://github.com/ammousa/TARLoco), and [üåê webpage](tarloco/). The presentation details will be updated soon!\n","tags":null,"title":"IROS 2025 Presentation","type":"news"},{"authors":null,"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"b39cb6b0b540c69d91b903e3ffc25efa","permalink":"","publishdate":"2023-02-01T00:00:00Z","relpermalink":"","section":"publication","summary":"**Amr Mousa**","tags":null,"title":"Extended-deep Q-network: A functional reinforcement learning-based energy management strategy for plug-in hybrid electric vehicles","type":"publication"},{"authors":null,"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"37210b0d9528b0c6a1cc7f9b791546f1","permalink":"","publishdate":"2023-01-01T00:00:00Z","relpermalink":"","section":"publication","summary":"Tomislav Bukic, **Amr Mousa**, Milan Zivadinovic, Bernhard Peischl, Harisyam Manda, Wolfgang Koenig, Armin Traussnig, Charles F. Gaylard","tags":[],"title":"Advanced Reinforcement Learning-Based Thermal Management Strategy For Battery Electric Vehicles","type":"publication"},{"authors":null,"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"506279f9891ee0e134782027d0a303bb","permalink":"","publishdate":"2022-12-01T00:00:00Z","relpermalink":"","section":"publication","summary":"**Amr Mousa**, Gerhard Benedikt Weiss","tags":[],"title":"Advanced Energy Management Strategies for Plug-In Hybrid Electric Vehicles via Deep Reinforcement Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"bdf6f76af3c5ec41db7ee4439e11c896","permalink":"","publishdate":"2022-05-01T00:00:00Z","relpermalink":"","section":"publication","summary":"**Amr Mousa**","tags":"","title":"Patent: Device and method for controlling a drivetrain of a hybrid vehicle","type":"publication"},{"authors":["Amr Mousa"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"dacf152564c128b1ade5edb905583983","permalink":"https://amrmousa.com/publication/thesis/fh_msc/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/thesis/fh_msc/","section":"publication","summary":"Plug-in Hybrid Electric Vehicles (PHEVs) offer a promising solution for the increasing CO2 emission problem. However, the improved economy of PHEVs strongly depends on the control strategy that decides on the power distribution between the Internal Combustion Engine (ICE) and the electric battery. Traditional rule-based control strategies are no more practical considering the increasing and more complex control objectives introduced by the emerging technologies such as automated driving and connected vehicles. In this study, an advanced Energy Management Strategy (EMS) based on Deterministic Dynamic Programming (DDP) and Reinforcement Learning (RL) is developed. DDP solves a finite-horizon optimization problem given the driving cycle a priori to obtain a global optimal vehicle power distribution that contributes mostly to the fuel economy improvements. DDP results are used to benchmark the subsequent RL-developed algorithms‚Äô performance. In the newly proposed control strategy, an adaptive online learning RL agent is introduced into the existing Hybrid Control Unit (HCU) architecture solving the EMS for near-optimal solutions. The objective is to minimize the vehicle's expected total fuel consumption with a proper battery depletion rate besides penalizing the frequent engine on/off switching. Several RL-based algorithms have been experimented with using a vehicle model simulation. As a result, an Extended Deep Q-Network (E-DQN) agent is proposed by the thesis, trained on one cycle, and deployed on two other cycles to evaluate the performance. The thesis findings showed that E-DQN outperformed the rule-based strategy achieving up to 10.46% improvement in fuel economy closer to the DP performance alongside providing adequate compliance with the vehicle drivability and driver com-fort objectives.","tags":["Plug-in Hybrid Electric Vehicles","Energy Management Strategy","Deterministic Dynamic Programming","Reinforcement Learning","Machine learning","Deep Neural Networks","Model-based RL"],"title":"AI-based Energy Management Strategies for P2 Plug-in Hybrid Electric Vehicles","type":"publication"},{"authors":["Amr Mousa"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"13f3331a4261bb8a554cd27840c8da6f","permalink":"https://amrmousa.com/publication/thesis/asu_meng/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/publication/thesis/asu_meng/","section":"publication","summary":"","tags":"","title":"Process design and implementation of an automated document issuance system for academic purposes","type":"publication"},{"authors":["Amr Mousa"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9f56ecf17e7dbc2fd5d75bddbc614e34","permalink":"https://amrmousa.com/publication/thesis/zewail_bsc/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/thesis/zewail_bsc/","section":"publication","summary":"The aerospace industry is moving fast towards saving money, keeping the environment green, reducing noise and minimizing emissions. In response to AIAA aircraft design competition (undergraduate-team category) ‚ÄúHybrid-Electric General Aviation (HEGAA) 2018‚Äù, hyBIRD aircraft family is presented in this design report. hyBIRD is a team of four undergraduate students from the University of Science and Technology at Zewail City, Egypt. The team worked on designing a family of aircraft for an entry in service (EIS) in 2028 for the 4-seater aircraft and 2030 for the 6-seater based on the future technology. This required forecasting the technology trends for the powertrain components and studying the similar airplanes market, in the same category, which helped identifying our competitive edge and selling point. Advanced design approaches were performed throughout the whole design process such as Integrated Reconfigurable Matrix of Alternatives (IRMA) for aircraft configuration selection, powertrain sizing optimization, Computational Fluid Dynamics (CFD) and composites FEA analysis. hyBIRD aircraft is an all-composite airframe with serial hybrid propulsion system utilizing a turbo-normalized piston engine, a frontal electric motor and two smaller motors on the wing-tips. A tadpole-shaped fuselage with low-wing and V-tail was selected. Both airplanes are identical except for the flight controller, motors and batteries which results in an 83% part-commonality by weight and this can help achieve minimal development, certification, and manufacturing costs. Based on the aircraft weight, complexity, and performance, the Eastlake cost model estimates the aircraft to be highly competitively to various aircraft of similar performance in terms of initial and operational costs.","tags":"","title":"hyBIRD: Design of a Four and Six Seater Hybrid-Electric Aircraft Family","type":"publication"},{"authors":null,"categories":null,"content":" T A R L o c o üêæ Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion üìÑ Paper üßæ arXiv üíª Code üìä W\u0026amp;B Results Amr Mousa1, Neil Karavis2, Michele Caprio1, Wei Pan1, Richard Allmendinger1 1 University of Manchester, UK ¬† | ¬† 2 BAE Systems, UK Conference: IROS 2025 (Accepted) üì¢ News 2025‚Äë08‚Äë18 ‚Äî Project website goes live with videos and results. 2025‚Äë06‚Äë15 ‚Äî Accepted at IROS 2025. üìå Abstract Quadrupedal locomotion via reinforcement learning (RL) is commonly addressed using the teacher‚Äìstudent paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between privileged teacher and proprioceptive‚Äëonly student, covariate shift due to behavioral cloning, and lack of deployable adaptation lead to poor generalization in real‚Äëworld scenarios. We propose Teacher‚ÄëAligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self‚Äësupervised contrastive learning to bridge this gap. By distilling from a privileged teacher in simulation and constructing structured latent spaces through contrastive objectives, our student policy surpasses the fully privileged ‚ÄúTeacher‚Äù and exhibits robust generalization to out‚Äëof‚Äëdistribution (OOD) scenarios. Results showed 2√ó faster training to reach peak performance compared to state‚Äëof‚Äëthe‚Äëart baselines, and ~40% better OOD generalization on average. Additionally, TAR transitions seamlessly into privileged‚Äëfree fine‚Äëtuning during deployment, enabling continual adaptation in the real world.\n3√ó Faster Convergence 7,500 Iterations to Peak +61% Return vs Baselines 103% Return vs Teacher Training Fine-tuning Results ‚öôÔ∏è Training Framework Overview The core idea of our method is to leverages contrastive learning to align latent representations between a privileged teacher and a proprioceptive student within RL paradigm. By structuring a shared latent space, the student utilizes the teacher‚Äôs privileged signals during training, enabling improved generalization and sim2real transfer. At deployment, the student operates with proprioception only, maintaining robust performance in diverse and dynamic environments.\nPipeline summary\nTeacher encoder consumes privileged states $S_t$ to produce structured embeddings $Z^{T}_t$. Student encoder consumes proprioceptive inputs $O_t$ and hidden state $h_{t-1}$ to produce $Z^{S}_t$. Contrastive alignment (triplet loss): the student‚Äôs next-state prediction $\\tilde{Z}^{+}_{t+1}$ is pulled towards the teacher‚Äôs future code $Z_{t+1}$ and away from negatives $Z^{-}_{t+1}$ sampled from other contexts. Policy optimization: actor‚Äìcritic is trained with policy gradients; the critic additionally leverages the contrastive signal for representation shaping. Velocity estimator: trained via regression and frozen post-training to stabilize deployment. Design goals\nRobust latent structure that transfers to diverse terrains and dynamics. Student policy that remains privileged-free at test time without performance collapse. Privileged-Free Fine-Tuning \u0026amp; Adaptation Self-supervised adaptation without access to the teacher. During adaptation (or privileged-free learning), the teacher encoder is removed:\nThe student forms positive/negative pairs from its own proprioceptive rollouts: Positives from temporally adjacent observations $O_{t+1}$ with consistent hidden state context. Negatives from other agents or distant contexts $O^{j\\neq i}_{t+1}$. This self-supervised contrastive sampling enforces temporal consistency and context separation without external supervision. The method is off-policy compatible, enabling efficient fine-tuning in dynamic, non-stationary environments. üìà Results üî¨ Training Protocol: All methods were trained under identical conditions in Isaacsim with curriculum learning, and domain randomization. Optimization used PPO with GAE over 20k iterations, and each experiment was repeated with 3 seeds. Training conducted in IsaacSim with 4096 parallel environments\nBaselines Hybrid Internal Model: HIMLoco . Self-learning Latent Representation: SLR. Teacher: Privileged expert policy with full state access (upper bound). Our Ablations Ours w/ MLP: Student encoder replaced by a 10-step MLP. Ours w/ TCN: Student encoder replaced by a temporal convolutional network. Ours w/o Priv: Same architecture but trained without privileged states. Ours w/o Priv Vel: No privileged states + no velocity inputs. üìä Click to Explore Interactive W\u0026amp;B Training Results Our method achieves:\n‚úÖ Faster convergence than HIM, SLR, and all ablations. ‚úÖ Higher final returns, surpassing even the privileged Teacher. ‚úÖ Robust OOD generalization, maintaining strong performance under terrain, friction, and payload shifts. Combined evaluation metrics for TAR, HIMLoco, and SLR and ablation varients. üé¨ Evaluation üî¨ Evaluation Protocol: Scenarios include both in-distribution cases (similar to training conditions) and challenging out-of-distribution scenarios ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"13ca20e0f4824eb2d93c0c2b21e2b455","permalink":"https://amrmousa.com/tarloco/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tarloco/","section":"","summary":"T A R L o c o üêæ Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion üìÑ Paper üßæ arXiv üíª Code üìä W\u0026B Results Amr Mousa1, Neil Karavis2, Michele Caprio1, Wei Pan1, Richard Allmendinger1 1 University of Manchester, UK ¬† | ¬† 2 BAE Systems, UK Conference: IROS 2025 (Accepted) üì¢ News 2025‚Äë08‚Äë18 ‚Äî Project website goes live with videos and results.","tags":null,"title":"","type":"page"}]